{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461743ad",
   "metadata": {},
   "source": [
    "# RAG Evaluation Using RAGAS\n",
    "\n",
    "## Introduction to Evaluation\n",
    "Evaluation in the context of Retrieval-Augmented Generation (RAG) systems involves assessing the performance of both the retrieval component (how well relevant documents are fetched from a knowledge base)\n",
    "and the generation component (how accurate, relevant, and coherent the generated responses are). A RAG system combines a retriever (e.g., a vector store like FAISS) with a language model (e.g., AzureChatOpenAI) to provide contextually informed responses, reducing issues like hallucinations (incorrect or fabricated information).\n",
    "\n",
    "## Why Do We Use Evaluation?\n",
    "- Evaluation is critical for the following reasons:\n",
    "- Quality Assurance: Ensures the RAG system delivers accurate, relevant, and trustworthy responses.\n",
    "- System Improvement: Identifies weaknesses in retrieval (e.g., irrelevant documents) or generation (e.g., unfaithful answers), guiding optimizations like better embeddings or prompt engineering.\n",
    "- Performance Monitoring: Quantifies system performance to track improvements or regressions over time.\n",
    "- Stakeholder Confidence: Provides metrics to demonstrate the system's reliability to stakeholders or end-users.\n",
    "\n",
    "### The RAGAS framework (Retrieval Augmented Generation Assessment) is used to evaluate RAG systems. It provides metrics like:\n",
    "- Faithfulness: Measures if the generated answer is factually grounded in the retrieved context.\n",
    "- Answer Relevancy: Assesses if the answer directly addresses the user's query.\n",
    "- Context Precision: Checks if the retrieved context contains relevant information with minimal noise.\n",
    "- Context Recall: Ensures all necessary information is retrieved (requires ground truth).\n",
    "\n",
    "This notebook sets up a RAG system using AzureChatOpenAI, AzureOpenAIEmbeddings, and FAISS, generates a synthetic test dataset, and evaluates the system using RAGAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b676455",
   "metadata": {},
   "source": [
    "Loads environment variables (e.g., API keys) from a .env file for secure configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da1a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -zure-cognitiveservices-speech (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -zure-cognitiveservices-speech (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -zure-cognitiveservices-speech (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -zure-cognitiveservices-speech (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -zure-cognitiveservices-speech (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -zure-cognitiveservices-speech (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pymupdf faiss-cpu ragas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0d1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gpt4o\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab08bef",
   "metadata": {},
   "source": [
    "\n",
    "Initializes AzureChatOpenAI for response generation and AzureOpenAIEmbeddings for creating document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db718d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(azure_deployment=MODEL_NAME)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "dir_path = r\"datasets/supply_chain\"\n",
    "index_path = r\"VectorDB_Chroma/faiss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f667d3",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "This section loads PDF documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b7b308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 131 document chunks.\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF documents from the specified directory using PyMuPDFLoader.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of loaded documents.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the directory does not exist.\n",
    "        Exception: For other loading errors.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {dir_path}\")\n",
    "    try:\n",
    "        loader = DirectoryLoader(dir_path, loader_cls=PyMuPDFLoader)\n",
    "        return loader.load()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of documents to split.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of document chunks. Returns empty list if no documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not documents:\n",
    "            return []\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "        return text_splitter.split_documents(documents)\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Load and split documents\n",
    "documents = load_documents()\n",
    "documents = split_documents(documents)\n",
    "print(f\"Loaded and split {len(documents)} document chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0726e",
   "metadata": {},
   "source": [
    "## Vector Store Creation\n",
    "\n",
    "Now splits documents into chunks, and creates a FAISS vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e80f2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vector Store...\n",
      "loaded successfully\n",
      "Vector store loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_vectorstore(documents):\n",
    "    \"\"\"\n",
    "    Create and save a new FAISS vector store from documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of document objects to convert to vectors.\n",
    "    \n",
    "    Returns:\n",
    "        None: If successful, else Exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(index_path, exist_ok=True)\n",
    "        vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        print(\"Vector Store created Successfully\")\n",
    "        save_vectorstore(vectorstore)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def save_vectorstore(vectorstore):\n",
    "    \"\"\"\n",
    "    Save the FAISS vector store to the specified path.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore (FAISS): The vector store to save.\n",
    "    \n",
    "    Returns:\n",
    "        None: If successful, else Exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vectorstore.save_local(index_path)\n",
    "        print(\"vector Store saved successfully\")\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def load_vectorstore():\n",
    "    \"\"\"\n",
    "    Load an existing FAISS vector store.\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Loaded vector store, else Exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"loading vector Store...\")\n",
    "        vs = FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"loaded successfully\")\n",
    "        return vs\n",
    "    \n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "# Load or create vector store\n",
    "if os.path.exists(index_path) and any(os.listdir(index_path)):\n",
    "    vectorstore = load_vectorstore()\n",
    "    vectorstore_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "else:\n",
    "    create_vectorstore(documents)\n",
    "    vectorstore = load_vectorstore()\n",
    "    print(vectorstore)\n",
    "    vectorstore_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "    print(\"Created and loaded new vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b579f6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- Document Loading: Uses DirectoryLoader with PyMuPDFLoader to load PDFs from the data directory.\n",
    "- Document Splitting: Splits documents into chunks (500 characters, 200 overlap) for efficient retrieval.\n",
    "- Vector Store: Creates a FAISS index from document embeddings or loads an existing one from the index directory.\n",
    "- Retriever: Configures the vector store as a retriever, fetching the top 5 relevant documents for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d66b",
   "metadata": {},
   "source": [
    "## RAG Chain Setup\n",
    "This section defines a RAG chain that validates queries, retrieves relevant documents, and generates answers using the AzureChatOpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1c3a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain Response:\n",
      "- A supply chain is a network of partners involved in converting a basic commodity into a finished product that adds value for end-customers.\n",
      "- It encompasses all processes from raw material production to distribution and recycling of the finished product.\n",
      "- Each partner in the supply chain performs processes that add value, transforming inputs (materials and information) into outputs (goods and services).\n",
      "- The management of a supply chain involves planning and controlling all business processes that link partners to meet the needs of the end-customer.\n",
      "- Supply chain management views the supply chain as a single entity and requires strategic decision-making and system integration.\n",
      "- The terms \"supply chain\" and \"supply network\" describe the connections between buyers and suppliers, with \"network\" indicating a more complex structure.\n",
      "- Effective supply chain management includes coordinating material flow and information flow across the entire network.\n",
      "- The ultimate goal of a supply chain is to ensure timely delivery of products that meet customer demands while minimizing costs and maximizing efficiency.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def validate_query(query):\n",
    "    \"\"\"\n",
    "    Validates a user's query by ensuring it is not empty and has at least 15 characters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "    \n",
    "    Returns:\n",
    "        str: The query if valid, or an error message if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not query:\n",
    "            return \"Query cannot be empty, enter a valid query.\"\n",
    "        elif len(query) < 15:\n",
    "            return \"Query is too short, enter a valid query.\"\n",
    "        else:\n",
    "            return query\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def create_rag_chain(query, relevant_documents):\n",
    "    \"\"\"\n",
    "    Creates and executes a RAG chain to answer a query using retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        relevant_documents (list): List of retrieved document chunks.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt_template = \"\"\"\n",
    "        Only based on the provided documents, answer the question in points. Do not mention from which document the answer is derived.\n",
    "        Question: {query}\n",
    "        Documents: {docs}\n",
    "        Note: You are a supply chain assistant. If the query is not related to supply chain or the documents do not provide the necessary information, return \"Invalid Query\".\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        valid_query = validate_query(query)\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        return rag_chain.invoke({\"query\": valid_query, \"docs\": relevant_documents})\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Test the RAG chain\n",
    "query = \"What is Supply Chain?\"\n",
    "relevant_documents = vectorstore_retriever.invoke(query)\n",
    "response = create_rag_chain(query, relevant_documents)\n",
    "print(\"RAG Chain Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf2f98",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- Query Validation: Ensures the query is non-empty and at least 15 characters long.\n",
    "- RAG Chain: Constructs a prompt that instructs the model to answer in bullet points, using only the retrieved documents, and to return \"Invalid Query\" if the query is unrelated to supply chain or unsupported by the documents.\n",
    "- Execution: Combines the prompt, AzureChatOpenAI model, and string output parser to generate a response.\n",
    "- Test: Runs a sample query to verify the RAG chain's functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2bbe2",
   "metadata": {},
   "source": [
    "## Generating Synthetic Test Data with RAGAS\n",
    "To evaluate the RAG system, we need a test dataset with questions, answers, contexts, and ground truth. RAGAS's TestsetGenerator can create synthetic data from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e18d305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anshu Pandey\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb38278074984ba3a9d168422dc2947a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4eaa236c7d4757977310c438c84c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a511045fa84ae3ac0d61b576c5b3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '249600'. Skipping!\n",
      "Property 'summary' already exists in node 'ba8d8f'. Skipping!\n",
      "Property 'summary' already exists in node '32c4c6'. Skipping!\n",
      "Property 'summary' already exists in node '2e055b'. Skipping!\n",
      "Property 'summary' already exists in node 'c406e9'. Skipping!\n",
      "Property 'summary' already exists in node '1ebfaa'. Skipping!\n",
      "Property 'summary' already exists in node 'd62a8c'. Skipping!\n",
      "Property 'summary' already exists in node '46486d'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37150d15bb92490a9b6949374a89b99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8803e29520440a581dc41517c3cee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying EmbeddingExtractor:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node 'd62a8c'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'ba8d8f'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '46486d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '1ebfaa'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '2e055b'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'c406e9'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '32c4c6'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '249600'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6f45bdbf2c4b0e893ec62eee9181db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying ThemesExtractor: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa90b2dedec3472b9898d925de4e3617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying NERExtractor: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526db107ab6647869560dfb74eb1b022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CosineSimilarityBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3338622ee934354a039c6f038baae66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5058d798478642af91d13d4f7f448b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d70972631644b84800f2a81a4b63fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0 test cases.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "# Configure the test set generator\n",
    "testset_generator = TestsetGenerator.from_langchain(\n",
    "    llm=llm,\n",
    "    embedding_model=embeddings\n",
    ")\n",
    "\n",
    "# Randomly sample a subset of documents (e.g., 50 out of 902 chunks)\n",
    "sample_size = 10  # Adjust based on your needs\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_documents = random.sample(documents, min(sample_size, len(documents)))\n",
    "\n",
    "# Generate test dataset with reduced test_size\n",
    "testset = testset_generator.generate_with_langchain_docs(sampled_documents, 5)\n",
    "\n",
    "# Convert test dataset to evaluation format\n",
    "eval_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": []\n",
    "}\n",
    "\n",
    "for testcase in testset.samples:\n",
    "    relevant_docs = vectorstore_retriever.invoke(testcase.eval_sample.user_input)\n",
    "    answer = create_rag_chain(testcase.eval_sample.user_input, relevant_docs)\n",
    "    eval_data[\"question\"].append(testcase.eval_sample.user_input)\n",
    "    eval_data[\"answer\"].append(answer)\n",
    "    eval_data[\"contexts\"].append([doc.page_content for doc in relevant_docs])\n",
    "    eval_data[\"ground_truth\"].append(testcase.eval_sample.reference)\n",
    "\n",
    "print(f\"Generated {len(eval_data['question'])} test cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ee10c",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- TestsetGenerator: Uses AzureChatOpenAI for generating and critiquing test cases, with AzureOpenAIEmbeddings for document embeddings.\n",
    "- Test Data Generation: Creates test cases with a mix of random samples\n",
    "- Evaluation Dataset: For each test case, retrieves relevant documents, generates an answer using the RAG chain, and collects the question, answer, contexts, and ground truth.\n",
    "- Output: Stores the data in a dictionary format suitable for RAGAS evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536cdd4",
   "metadata": {},
   "source": [
    "## RAG Evaluation with RAGAS\n",
    "This section evaluates the RAG system using RAGAS metrics: faithfulness, answer relevancy, context precision, and context recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac8b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anshu Pandey\\AppData\\Local\\Temp\\ipykernel_5332\\3393354811.py:12: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m evaluator_embeddings \u001b[38;5;241m=\u001b[39m LangchainEmbeddingsWrapper(embeddings)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Checks if the answer is grounded in the context\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Checks if the answer addresses the question\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Checks if retrieved context is relevant\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_recall\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Checks if all necessary information is retrieved\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Print evaluation results\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAGAS Evaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ragas\\_analytics.py:277\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    276\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 277\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ragas\\evaluation.py:195\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[0m\n\u001b[0;32m    192\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m EvaluationDataset\u001b[38;5;241m.\u001b[39mfrom_list(dataset\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[1;32m--> 195\u001b[0m     \u001b[43mvalidate_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     validate_supported_metrics(dataset, metrics)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ragas\\validation.py:58\u001b[0m, in \u001b[0;36mvalidate_required_columns\u001b[1;34m(ds, metrics)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_required_columns\u001b[39m(ds: EvaluationDataset, metrics: t\u001b[38;5;241m.\u001b[39mSequence[Metric]):\n\u001b[1;32m---> 58\u001b[0m     metric_type \u001b[38;5;241m=\u001b[39m \u001b[43mget_supported_metric_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m     60\u001b[0m         required_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(m\u001b[38;5;241m.\u001b[39mrequired_columns\u001b[38;5;241m.\u001b[39mget(metric_type, []))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ragas\\validation.py:48\u001b[0m, in \u001b[0;36mget_supported_metric_type\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_supported_metric_type\u001b[39m(ds: EvaluationDataset):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    get the supported metric type for the given dataset\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     sample_type \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sample_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_type \u001b[38;5;241m==\u001b[39m SingleTurnSample:\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MetricType\u001b[38;5;241m.\u001b[39mSINGLE_TURN\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ragas\\dataset_schema.py:205\u001b[0m, in \u001b[0;36mRagasDataset.get_sample_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_sample_type\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mType[Sample]:\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the type of the samples in the dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import Dataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Convert evaluation data to Hugging Face Dataset\n",
    "eval_dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "# Wrap AzureChatOpenAI for RAGAS compatibility\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,       # Checks if the answer is grounded in the context\n",
    "        answer_relevancy,   # Checks if the answer addresses the question\n",
    "        context_precision,  # Checks if retrieved context is relevant\n",
    "        context_recall      # Checks if all necessary information is retrieved\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"RAGAS Evaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed496a",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- Dataset Conversion: Converts the evaluation data into a Hugging Face Dataset for RAGAS.\n",
    "- LLM Wrapper: Wraps AzureChatOpenAI with LangchainLLMWrapper for compatibility with RAGAS.\n",
    "- Metrics: Evaluates the RAG system on:\n",
    "    - Faithfulness: Ensures answers are factually consistent with the context.\n",
    "    - Answer Relevancy: Measures how well answers address the query.\n",
    "    - Context Precision: Assesses the relevance of retrieved documents.\n",
    "    - Context Recall: Checks if all necessary information is retrieved (uses ground truth).\n",
    "- Results: Outputs scores (0 to 1) for each metric, where higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2a7e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
