{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eW9gZ8lMlON"
      },
      "source": [
        "# Prompt Engineering for NLP Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BdtUFWkkHw05"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "client = AzureOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0WAxkVWZ7LRi"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "\n",
        "def get_response(prompt,model=\"gpt4o\",temperature=0.0,max_tokens=300):\n",
        "  messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_tokens\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLiyOECFIfFL"
      },
      "source": [
        "## Prompt Engineering for Inferense and prediction with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DDTd9S_KIlZf"
      },
      "outputs": [],
      "source": [
        "review = \"\"\"\n",
        "The HP Envy x360 15 is a premium convertible laptop that offers a perfect blend of performance, portability, and versatility. It's powered by up to an Intel Core i7-13800H processor, 32GB of RAM, and a 1TB SSD, making it a powerhouse that can handle any task you throw at it, from demanding productivity workloads to intensive creative projects and AAA gaming.\n",
        "The Envy x360 15 also features a stunning 15.6-inch touchscreen display with a resolution of 1920x1080 pixels. The display is bright, sharp, and color-accurate, making it ideal for watching movies, editing photos and videos, or simply browsing the web.\n",
        "Thanks to its convertible design, the Envy x360 15 can be used in a variety of different modes, including laptop mode, tablet mode, tent mode, and stand mode. This makes it ideal for students, professionals, and creatives who need a laptop that can adapt to their changing needs.\n",
        "The Envy x360 15 also comes with a number of other premium features, including a backlit keyboard, a fingerprint sensor, a webcam with privacy shutter, Bang & Olufsen speakers, and a variety of ports, including Thunderbolt 4, USB-C, and HDMI.\n",
        "Overall, the HP Envy x360 15 is a fantastic convertible laptop that offers a great combination of performance, portability, versatility, and features. If you're looking for a premium convertible laptop that can do it all, the Envy x360 15 is a great choice.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENZHEK_9Je3r"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVT2oDbYJhQW",
        "outputId": "ee8bbdd7-e840-4eea-f40a-3292c2200800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sentiment of the review is overwhelmingly positive. The reviewer highlights the HP Envy x360 15's performance, portability, versatility, and premium features, expressing satisfaction with its capabilities and design. Phrases like \"perfect blend,\" \"powerhouse,\" \"stunning display,\" and \"fantastic convertible laptop\" contribute to the overall positive sentiment. The review concludes with a strong recommendation, indicating that it is a great choice for potential buyers.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\" What is the senitment for the below review delimited by triple backticks:\n",
        "Review: ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_HbKtK7JrSl",
        "outputId": "5f571d64-0cd1-4088-c57b-514a36205daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\" What is the senitment for the below review delimited by triple backticks.\n",
        "\n",
        "Provide the output as a single word e.g. 'Positive' or 'Negative'\n",
        "\n",
        "Review: ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py8KMLpaKzeT"
      },
      "source": [
        "### Extracting emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTU84XGCKIkn",
        "outputId": "e4824710-6938-4d93-d9be-c2dc8b7e122e"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Identify a list of emotions the user is expressing while writing the below reivew separated by triple backticks.\n",
        "Provide output as a list of emotions in lowercase separated by comma. provide output in python list format.\n",
        "Review: ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swDLa36uK0vV"
      },
      "source": [
        "### Identifying anger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7EVWQdzKlip",
        "outputId": "20fc444b-ac81-4113-9679-701680ecf95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Whether the user is expresssing anger or not in the below reivew separated by triple backticks.\n",
        "Provide output as yes or no\n",
        "Review: ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT14kM8PLOE0"
      },
      "source": [
        "### Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsCg0bm8K-56",
        "outputId": "dcc238fc-d9ea-4b23-898a-9bf53759d72b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"item\": \"HP Envy x360 15\",\n",
            "  \"brand\": \"HP\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the key entities given below from the provided review separated by triple backticks.\n",
        "1. Name of Product mentioned in the review\n",
        "2. Name of company that made the item\n",
        "\n",
        "\n",
        "Provide output as a JSON value, use keys such as 'item' and 'brand'. if the entities are not present, use the word 'unknown' for the value of respective key.\n",
        "keep the result as short as possible.\n",
        "Review: ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZhJV_XkUH2w"
      },
      "source": [
        "### Asking for mulitple info at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZsGoO9DL5my",
        "outputId": "f771957d-7bba-4e15-beee-d0983816ae07"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mIdentify items given below from the provided review separated by triple backticks.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m1. Sentiment (positive/negative)\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mReview: ```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(prompt, model, temperature, max_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_response\u001b[39m(prompt,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m      4\u001b[0m   messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt}]\n\u001b[1;32m----> 5\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mTypeError\u001b[0m: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Identify items given below from the provided review separated by triple backticks.\n",
        "1. Sentiment (positive/negative)\n",
        "2. Is user expressing anger (yes/no)\n",
        "3. A list of emotions user is expressing while writing review (lowercase, comma separated, in a list [ ])\n",
        "4. Item purchased by the user\n",
        "5. Name of company that made the item\n",
        "\n",
        "\n",
        "Provide output as a JSON value, use keys such as 'sentiment','anger','emotions','item' and 'brand'. if the entities are not present, use the word 'unknown' for the value of respective key.\n",
        "keep the result as short as possible.\n",
        "Review: ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it-kYeuPUKsg"
      },
      "source": [
        "### Topic Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o44-M3aZT62n"
      },
      "outputs": [],
      "source": [
        "article = \"\"\"\n",
        "Israeli Prime Minister Benjamin Netanyahu urged Elon Musk to strike a balance between protecting free expression and fighting hate speech at a meeting on Monday after weeks of controversy over antisemitic content on Mr. Musk's social media platform X.\n",
        "Earlier this month, Mr. Musk attacked the Anti-Defamation League, accusing the nonprofit that works to fight antisemitism of primarily causing a 60% decrease in U.S. ad revenue at X, without providing evidence.\n",
        "Mr. Musk bought the platform, then known as Twitter, in October.\n",
        "Mr. Musk previously joined a conversation on X with the hashtag #BantheADL, engaging with users who expressed white supremacist views, and asked followers whether he should poll the platform about banning the ADL.\n",
        "\"I hope you find within the confines of the First Amendment, the ability to not only stop antisemitism... but any collective hatred of a people,\" Mr. Netanyahu said during the meeting that was broadcast live on X from Tesla's factory in Fremont, California.\n",
        "\"I know you're committed to that... but I encourage and urge you to find a balance,\" Mr. Netanyahu said.\n",
        "Mr. Musk responded by saying he was against antisemitism and against anything that \"promotes hate and conflict,\" repeating his previous statements that X would not promote hate speech.\n",
        "Mr. Musk has said X should be a platform for people to post diverse viewpoints, but the company will limit the distribution of certain posts that may violate its policies, calling the approach \"freedom of speech, not reach.\"\n",
        "The billionaire, who also runs Tesla and SpaceX, noted that he received more pushback from Tesla employees about the meeting with Mr. Netanyahu than \"anything else I've ever done.\"\n",
        "Mr. Netanyahu and his nationalist-religious coalition are trying to limit some of the Israeli Supreme Court's powers, arguing it is necessary to prevent political overreach by unelected judges.\n",
        "Opponents say the changes could encourage corruption and abuses of power by removing effective oversight, and the issue has split Israeli society and raised concerns over Israel's democratic health.\n",
        "About 200 people protesting the judicial overhaul gathered outside Tesla's California factory, where the event was held.\n",
        "Mr. Musk and Mr. Netanyahu also discussed how to harness the benefits of the rapid advancement of artificial intelligence, while limiting the risks to society, a concern Mr. Musk and others in the tech industry have raised in recent months.\n",
        "\"We stand today at a juncture for all humanity, where we have to choose between a blessing and a curse,\" Mr. Netanyahu said, adding that AI could advance medicine but lead to risks like disrupting democracy.\n",
        "Israel is considered a world-leader in AI, thanks to burgeoning computing and robotics industries that draw on talent developed in the technologically advanced conscript military.\n",
        "Foreign investment in Israeli tech startups has plunged in the last year, partly due to a global slowdown and exacerbated by investor fears that the push to trim the Supreme Court's powers would remove a key check and balance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNXW7OvvU4ci",
        "outputId": "5455a9b8-8200-4660-95eb-03632f686fd1"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mIdentify the 5 key topics which are being discussed in the below reivew separated by triple backticks.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mMake each item no longer than one or two words.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mReview: ```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(prompt, model, temperature, max_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_response\u001b[39m(prompt,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m      4\u001b[0m   messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt}]\n\u001b[1;32m----> 5\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mTypeError\u001b[0m: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the 5 key topics which are being discussed in the below reivew separated by triple backticks.\n",
        "Make each item no longer than one or two words.\n",
        "\n",
        "Provide output list of topics, lowercase, comma separated\n",
        "Review: ```{article}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybUJ8mAk6V6C",
        "outputId": "fb71bcc5-6017-4f81-f485-e12f8ab5c562"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mIdentify the key people, key locations, name of organizations and key topics which are being discussed in the below reivew separated by triple backticks.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mMake each item no longer than one or two words.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mReview: ```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(prompt, model, temperature, max_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_response\u001b[39m(prompt,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m      4\u001b[0m   messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt}]\n\u001b[1;32m----> 5\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mTypeError\u001b[0m: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the key people, key locations, name of organizations and key topics which are being discussed in the below reivew separated by triple backticks.\n",
        "Make each item no longer than one or two words.\n",
        "\n",
        "Provide output list of topics, lowercase, comma separated for each category in JSON format with keys: people, orgs, locations, topics.\n",
        "Review: ```{article}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi-IhWDyVavb"
      },
      "source": [
        "### Language Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqzlvNLkVPjG",
        "outputId": "68ac340a-3721-4722-9a70-ffde1d1b86b1"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mTranslate below text into indonesian separated by triple backticks.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mText: ```Hello, How are you?```\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(prompt, model, temperature, max_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_response\u001b[39m(prompt,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m      4\u001b[0m   messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt}]\n\u001b[1;32m----> 5\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mTypeError\u001b[0m: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Translate below text into indonesian separated by triple backticks.\n",
        "Text: ```Hello, How are you?```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVfG0IsVVrgA",
        "outputId": "d04af6ed-20e0-4c20-d2b3-fda5708847be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "Bonjour, comment ça va ?\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Translate below text into french separated by triple backticks.\n",
        "Text: ```Hello, How are you?```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz0GVTVsVv1F",
        "outputId": "fa4c559d-e4f4-4f8c-8e63-64b26e41e4c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text \"Terima Kasih\" is in Indonesian.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the language for the text separated by triple backticks.\n",
        "Text: ```Terima Kasih```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AbYP_jZV416",
        "outputId": "a1f58e1a-5eb9-4cd8-c2c4-492bb3783efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "French:\n",
            "```\n",
            "Bonjour, comment ça va ?\n",
            "```\n",
            "\n",
            "Hindi:\n",
            "```\n",
            "नमस्ते, आप कैसे हैं?\n",
            "```\n",
            "\n",
            "Arabic:\n",
            "```\n",
            "مرحبًا، كيف حالك؟\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Translate below text into french, hindi and arabic separated by triple backticks.\n",
        "Text: ```Hello, How are you?```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-sF3VyOWBwu",
        "outputId": "2c863cdc-8670-475d-c6aa-245843f3d40c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formal:\n",
            "```\n",
            "Helo, Apa khabar anda?\n",
            "```\n",
            "\n",
            "Informal:\n",
            "```\n",
            "Hai, Apa khabar?\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = f\"\"\"\n",
        "Translate below text into malay both in formal and informal way separated by triple backticks.\n",
        "Text: ```Hello, How are you?```\n",
        "\"\"\"\n",
        "\n",
        "response = get_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVdvwN5MfTgM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APhWWteG-yih"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
