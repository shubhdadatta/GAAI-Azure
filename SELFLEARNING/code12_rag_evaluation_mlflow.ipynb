{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da88a93b",
   "metadata": {},
   "source": [
    "\n",
    "# LLM RAG Evaluation with MLflow Example Notebook\n",
    "\n",
    "This notebook demonstrates how to evaluate a **Retrieval‑Augmented Generation (RAG)** system built with [LangChain](https://python.langchain.com/) and [Chroma](https://www.trychroma.com/) using **MLflow's GenAI evaluation** capabilities. It follows the [MLflow GenAI documentation example](https://mlflow.org/docs/3.1.3/genai/eval-monitor/notebooks/rag-evaluation/) for version 3.1.3.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Set up the environment and note the recommended package versions.\n",
    "- Build a simple RAG system that answers questions about the MLflow documentation using LangChain and Chroma.\n",
    "- Define custom evaluation metrics for *faithfulness* and *relevance* using `mlflow.metrics.genai`.\n",
    "- Evaluate the RAG system on a small set of questions with `mlflow.evaluate()` and inspect the results.\n",
    "\n",
    "> **Note:** To run this notebook successfully, you must have valid API credentials for OpenAI. Set your `OPENAI_API_KEY` (and optional Azure variables) as environment variables before running the cells that interact with the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc7c64",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "The following table lists the package versions recommended by the MLflow documentation. Using these versions ensures compatibility with this example. Newer versions may also work, but if you encounter issues, try reverting to these:\n",
    "\n",
    "| Package               | Version |\n",
    "|----------------------|---------|\n",
    "| `langchain`           | 0.1.16  |\n",
    "| `langchain-community` | 0.0.33  |\n",
    "| `langchain-openai`    | 0.0.8   |\n",
    "| `openai`             | 1.12.0  |\n",
    "| `mlflow`             | 2.12.1  |\n",
    "| `chromadb`           | 0.4.24  |\n",
    "\n",
    "Install the packages (if they are not already installed) via `pip`:\n",
    "\n",
    "```bash\n",
    "pip install langchain==0.1.16 langchain-community==0.0.33 langchain-openai==0.0.8 openai==1.12.0 mlflow==2.12.1 chromadb==0.4.24\n",
    "```\n",
    "\n",
    "### Setting API credentials\n",
    "\n",
    "MLflow and LangChain rely on language model providers such as OpenAI.\n",
    "To authenticate with OpenAI, set the following environment variable in your shell **before** starting your notebook kernel:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"<YOUR-OPENAI-API-KEY>\"\n",
    "```\n",
    "\n",
    "If you are using Azure OpenAI, you may need additional variables:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_TYPE=\"azure\"\n",
    "export OPENAI_API_VERSION=\"<YYYY-MM-DD>\"\n",
    "export OPENAI_API_KEY=\"<YOUR-AZURE-OPENAI-KEY>\"\n",
    "export OPENAI_API_DEPLOYMENT_NAME=\"<DEPLOYMENT-NAME>\"\n",
    "```\n",
    "\n",
    "Once the environment variables are set, restart your notebook kernel or re‑run the cell below to load them into `os.environ`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# Load the OPENAI_API_KEY from environment variables. If the key is set in your environment\n",
    "# this cell will print confirmation. Otherwise, set it manually as shown below.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "\n",
    "print(\"OPENAI_API_KEY is set\" if os.environ.get(\"OPENAI_API_KEY\") else \"OPENAI_API_KEY is not set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e8332",
   "metadata": {},
   "source": [
    "\n",
    "## Create a RAG system\n",
    "\n",
    "A **Retrieval‑Augmented Generation (RAG)** system combines a language model with a document retrieval component. In this example, we:\n",
    "\n",
    "1. **Load the MLflow documentation** using LangChain's `WebBaseLoader`.\n",
    "2. **Split the documents** into manageable chunks using `CharacterTextSplitter`.\n",
    "3. **Embed the text** with OpenAI embeddings via `OpenAIEmbeddings`.\n",
    "4. **Store and search** embeddings using a `Chroma` vector store.\n",
    "5. **Create a `RetrievalQA` chain** that uses the vector store as a retriever and OpenAI as the language model.\n",
    "\n",
    "This setup allows the model to search the documentation for relevant passages before answering questions, improving factual accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "import mlflow\n",
    "\n",
    "# 1. Load the MLflow documentation\n",
    "loader = WebBaseLoader(\"https://mlflow.org/docs/latest/index.html\")\n",
    "# This call fetches the webpage and parses its content\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Create embeddings for the text\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 4. Build a Chroma vector store from the embeddings (in memory by default)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# 5. Construct a RetrievalQA chain using an OpenAI language model\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed372b21",
   "metadata": {},
   "source": [
    "\n",
    "## Wrap the RAG chain in a model function\n",
    "\n",
    "To make our RAG chain compatible with MLflow's evaluation interface, we wrap it in a simple function that accepts a pandas DataFrame of inputs (with a `questions` column) and returns a list of results. Each result contains the model's answer and the source documents retrieved by the chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(input_df: pd.DataFrame):\n",
    "    '''Run the RAG pipeline on each row of the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : pd.DataFrame\n",
    "        A DataFrame containing a 'questions' column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of results from the RetrievalQA chain, including answers and source docs.\n",
    "    '''\n",
    "    answers = []\n",
    "    for _, row in input_df.iterrows():\n",
    "        answers.append(qa(row[\"questions\"]))\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd668fa6",
   "metadata": {},
   "source": [
    "\n",
    "## Create an evaluation dataset\n",
    "\n",
    "We'll define a small evaluation dataset containing questions about MLflow. You can expand or modify this list to test your RAG system more thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"questions\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"How to run mlflow.evaluate()?\",\n",
    "            \"How to log_table()?\",\n",
    "            \"How to load_table()?\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "# Display the evaluation DataFrame\n",
    "eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ff10d",
   "metadata": {},
   "source": [
    "\n",
    "## Define a faithfulness metric\n",
    "\n",
    "**Faithfulness** measures how consistent the model's output is with the provided context. We create a list of `EvaluationExample` objects containing an input question, the model's output, a score (1–5), and a justification. These examples help the metric calibrate how to rate future outputs.\n",
    "\n",
    "The scoring rubric for faithfulness is summarized below:\n",
    "\n",
    "- **1:** None of the output is supported by the context.\n",
    "- **2:** A minority of claims are supported; most are unsupported or incorrect.\n",
    "- **3:** Roughly half of the output is supported.\n",
    "- **4:** Most claims are supported, with little extraneous information.\n",
    "- **5:** All claims are directly supported by the context.\n",
    "\n",
    "You can inspect the full grading prompt by printing the metric or accessing its `metric_details` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa95f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.metrics.genai import EvaluationExample, faithfulness\n",
    "\n",
    "# Create two reference examples to calibrate faithfulness\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=(\n",
    "            \"mlflow.autolog(disable=True) will disable autologging for all functions. \"\n",
    "            \"In Databricks, autologging is enabled by default.\"\n",
    "        ),\n",
    "        score=2,\n",
    "        justification=(\n",
    "            \"The output provides a working solution using the mlflow.autolog() function, \"\n",
    "            \"but includes extra information not fully supported by the context.\"\n",
    "        ),\n",
    "        grading_context={\n",
    "            \"context\": (\n",
    "                \"mlflow.autolog(log_input_examples: bool = False, \"\n",
    "                \"log_model_signatures: bool = True, log_models: bool = True, \"\n",
    "                \"log_datasets: bool = True, disable: bool = False, exclusive: bool = False, \"\n",
    "                \"disable_for_unsupported_versions: bool = False, silent: bool = False, \"\n",
    "                \"extra_tags: Optional[Dict[str, str]] = None) → None: \"\n",
    "                \"Enables (or disables) and configures autologging for all supported integrations.\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n",
    "        score=5,\n",
    "        justification=(\n",
    "            \"The output correctly identifies the function call needed to disable autologging \"\n",
    "            \"without adding unsupported details.\"\n",
    "        ),\n",
    "        grading_context={\n",
    "            \"context\": (\n",
    "                \"mlflow.autolog(log_input_examples: bool = False, \"\n",
    "                \"log_model_signatures: bool = True, log_models: bool = True, \"\n",
    "                \"log_datasets: bool = True, disable: bool = False, exclusive: bool = False, \"\n",
    "                \"disable_for_unsupported_versions: bool = False, silent: bool = False, \"\n",
    "                \"extra_tags: Optional[Dict[str, str]] = None) → None: \"\n",
    "                \"Enables (or disables) and configures autologging for all supported integrations.\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\n",
    "faithfulness_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5875ba",
   "metadata": {},
   "source": [
    "\n",
    "## Define a relevance metric\n",
    "\n",
    "**Relevance** assesses how well the model's answer addresses the user's question given the provided context. The metric uses a rubric similar to faithfulness, focusing on the alignment between the input question and the output answer.\n",
    "\n",
    "For reference, a high relevance score requires that the answer fully and accurately addresses the question using the contextual information. A low score indicates irrelevance or off‑topic content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.metrics.genai import relevance\n",
    "\n",
    "# Create a relevance metric using the same base model\n",
    "relevance_metric = relevance(model=\"openai:/gpt-4\")\n",
    "relevance_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb7005",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluate the RAG system with MLflow\n",
    "\n",
    "We can now evaluate our RAG system using `mlflow.evaluate()`.\n",
    "\n",
    "- `model`: our wrapper function that runs the retrieval and generation.\n",
    "- `eval_df`: the dataset of questions.\n",
    "- `model_type`: set to `\"question-answering\"` so MLflow uses the appropriate evaluator.\n",
    "- `evaluators`: set to `\"default\"` to use MLflow's built‑in evaluator for question‑answering.\n",
    "- `predictions`: the column name where predictions will be stored in the result.\n",
    "- `extra_metrics`: a list of additional metrics to compute; we include our custom `faithfulness_metric` and `relevance_metric`, as well as the built‑in `mlflow.metrics.latency()`.\n",
    "- `evaluator_config`: maps our DataFrame columns to the expected input and context fields used by the evaluator.\n",
    "\n",
    "This step may take several minutes and will make calls to the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = mlflow.evaluate(\n",
    "    model,\n",
    "    eval_df,\n",
    "    model_type=\"question-answering\",\n",
    "    evaluators=\"default\",\n",
    "    predictions=\"result\",\n",
    "    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"questions\",\n",
    "            \"context\": \"source_documents\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "# Display aggregated metrics\n",
    "results.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the detailed per‑example results table\n",
    "results.tables[\"eval_results_table\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4096dc",
   "metadata": {},
   "source": [
    "\n",
    "## Interpret the results\n",
    "\n",
    "The output of `mlflow.evaluate()` includes two components:\n",
    "\n",
    "1. **Aggregated metrics** (`results.metrics`): statistical summaries such as mean, variance, and 90th percentile (p90) for each metric (toxicity, readability scores, faithfulness, relevance, etc.). These help you understand overall performance.\n",
    "\n",
    "2. **Detailed table** (`results.tables[\"eval_results_table\"]`): per‑question results that include the model's answer, the retrieved source documents, latency, token counts, and the scores/justifications for each custom metric. Reviewing this table helps diagnose specific strengths and weaknesses of your RAG system.\n",
    "\n",
    "You can log these results to an MLflow experiment for experiment tracking and comparison across different models or retrieval strategies.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
