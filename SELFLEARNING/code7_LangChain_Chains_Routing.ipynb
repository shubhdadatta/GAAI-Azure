{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluedata-Consulting/GAAPB01-training-code-base/blob/main/SELFLEARNING_LangChain_Chains_Routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031a0acd",
      "metadata": {},
      "source": [
        "# Chains - Routing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b47436a",
      "metadata": {
        "id": "4b47436a"
      },
      "source": [
        "# How to route between sub-chains\n",
        "\n",
        ":::info Prerequisites\n",
        "\n",
        "This guide assumes familiarity with the following concepts:\n",
        "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
        "- [Chaining runnables](/docs/how_to/sequence/)\n",
        "- [Configuring chain parameters at runtime](/docs/how_to/configure)\n",
        "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
        "- [Chat Messages](/docs/concepts/#message-types)\n",
        "\n",
        ":::\n",
        "\n",
        "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.\n",
        "\n",
        "There are two ways to perform routing:\n",
        "\n",
        "1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions) (recommended)\n",
        "2. Using a `RunnableBranch` (legacy)\n",
        "\n",
        "We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c6edac",
      "metadata": {
        "id": "c1c6edac"
      },
      "source": [
        "## Example Setup\n",
        "First, let's create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e7be8062",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name='gpt4o'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8a8a1967",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8a8a1967",
        "outputId": "13bbe756-f42c-45dc-b36c-ca89e7c61d69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Gemini'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "model = init_chat_model(model_name, model_provider=\"azure_openai\")\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Gemini`, or `Other`.\n",
        "\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"how do I call Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rhpM0hSvPe4J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rhpM0hSvPe4J",
        "outputId": "e9a5d8d3-fa84-449d-d550-0466c3462df6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChain'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KjaNGHrGPivP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "KjaNGHrGPivP",
        "outputId": "7c01fff5-4ba5-4019-c91c-97656cd2771d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Other'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"what are the models available with OpenAI API?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7655555f",
      "metadata": {
        "id": "7655555f"
      },
      "source": [
        "Now, let's create three sub chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "89d7722d",
      "metadata": {
        "id": "89d7722d"
      },
      "outputs": [],
      "source": [
        "langchain_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in langchain. \\\n",
        "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "\n",
        "gemini_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in Vertex AI Gemini Models. \\\n",
        "Always answer questions starting with \"As Sundar Pichai told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "general_chain = PromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8d042c",
      "metadata": {
        "id": "6d8d042c"
      },
      "source": [
        "## Using a custom function (Recommended)\n",
        "\n",
        "You can also use a custom function to route between different outputs. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "687492da",
      "metadata": {
        "id": "687492da"
      },
      "outputs": [],
      "source": [
        "def route(info):\n",
        "    if \"gemini\" in info[\"topic\"].lower():\n",
        "        return gemini_chain\n",
        "    elif \"langchain\" in info[\"topic\"].lower():\n",
        "        return langchain_chain\n",
        "    else:\n",
        "        return general_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "02a33c86",
      "metadata": {
        "id": "02a33c86"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
        "    route\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c2e977a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e977a4",
        "outputId": "83c00a3b-e0a9-4297-eff2-4d7182117a63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, using Gemini involves a few key steps. First, you'll want to access the Vertex AI platform through the Google Cloud console. Once you're there, you can create a new project or select an existing one.\\n\\nNext, you can choose the specific Gemini model that fits your needs, whether it's for text generation, image processing, or other AI tasks. You'll need to set up your environment, which includes configuring the necessary APIs and permissions.\\n\\nAfter that, you can start experimenting with the model by feeding it data and observing the outputs. The platform also offers tools for fine-tuning the models based on your specific use cases, along with detailed documentation and support to help you get the most out of Gemini.\\n\\nRemember to monitor your usage and optimize your model configurations as you go along to ensure efficient performance. Happy experimenting!\", response_metadata={'token_usage': {'completion_tokens': 170, 'prompt_tokens': 48, 'total_tokens': 218, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-e1cf28f3-f297-4209-a628-4350607215ad-0', usage_metadata={'input_tokens': 48, 'output_tokens': 170, 'total_tokens': 218})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "48913dc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48913dc6",
        "outputId": "b6d54820-821b-4d28-e833-a7a2c4666d19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Harrison Chase told me, using LangChain involves several key steps to effectively harness its capabilities. First, you need to install the LangChain library via pip. Once installed, you can start by setting up your environment, which may include configuring any necessary API keys for the language models you plan to use.\\n\\nNext, you'll want to define your chain components, which can include various models, tools, or memory elements depending on your application's needs. LangChain allows you to create different types of chains, such as LLM chains, simple sequential chains, or more complex multi-step chains.\\n\\nAfter defining your components, you can execute the chain to process your input and obtain the desired output. Throughout this process, you can also leverage built-in tools for handling prompts, managing memory, and integrating with external APIs or data sources.\\n\\nLastly, it's beneficial to explore the extensive documentation and examples provided by LangChain to learn about advanced features and best practices. This will help you tailor the library to your specific use case and enhance your application's functionality.\", response_metadata={'token_usage': {'completion_tokens': 206, 'prompt_tokens': 44, 'total_tokens': 250, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-a6f6762e-bb75-4116-aa7a-5160301819d6-0', usage_metadata={'input_tokens': 44, 'output_tokens': 206, 'total_tokens': 250})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a14d0dca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a14d0dca",
        "outputId": "c5f2fff8-00c9-41b1-8293-39e5d3cc0719"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 equals 4.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 24, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-0d057254-6696-4351-ae2c-67c367139dd8-0', usage_metadata={'input_tokens': 24, 'output_tokens': 9, 'total_tokens': 33})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5147b827",
      "metadata": {
        "id": "5147b827"
      },
      "source": [
        "## Using a RunnableBranch\n",
        "\n",
        "A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does **not** offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\n",
        "\n",
        "A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
        "\n",
        "If no provided conditions match, it runs the default runnable.\n",
        "\n",
        "Here's an example of what it looks like in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2a101418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a101418",
        "outputId": "eb836e77-3875-4b70-fcf8-61db67f31701"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, to use Gemini, you should start by accessing the Google Cloud Platform and navigating to the Vertex AI section. From there, you can set up your environment, choose the appropriate Gemini model for your specific use case, and utilize the provided APIs to integrate it into your applications. Be sure to refer to the detailed documentation for examples and best practices tailored to your needs. Additionally, exploring the user interface can help you fine-tune parameters and evaluate the model's performance effectively.\", response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 48, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-eab6c8d0-180d-4579-8d07-c7d7496ebdfe-0', usage_metadata={'input_tokens': 48, 'output_tokens': 103, 'total_tokens': 151})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"gemini\" in x[\"topic\"].lower(), gemini_chain),\n",
        "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
        "    general_chain,\n",
        ")\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8d8caf9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d8caf9b",
        "outputId": "03f64370-3c05-4adb-c463-27101833fa2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='As Harrison Chase told me, using LangChain involves several key steps to help you build applications that leverage language models effectively. Here’s a concise guide to get you started:\\n\\n1. **Installation**: First, you need to install the LangChain library. You can do this using pip:\\n   ```bash\\n   pip install langchain\\n   ```\\n\\n2. **Set Up Your Environment**: Make sure you have access to a language model, whether it\\'s OpenAI\\'s GPT, Hugging Face models, or others. You\\'ll also want to configure your API keys if required.\\n\\n3. **Import Necessary Modules**: Start by importing the components you need from LangChain. This may include models, chains, or tools depending on your application.\\n\\n4. **Initialize the Model**: Create an instance of the language model you want to use. For example, if you\\'re using OpenAI’s GPT model:\\n   ```python\\n   from langchain.llms import OpenAI\\n   llm = OpenAI(api_key=\"your_api_key\")\\n   ```\\n\\n5. **Create Chains**: Chains are sequences of calls to different components. You can set up a simple chain to process inputs and generate outputs:\\n   ```python\\n   from langchain.chains import LLMChain\\n   chain = LLMChain(llm=llm, prompt=\"What is the capital of France?\")\\n   response = chain.run()\\n   ```\\n\\n6. **Utilize Tools and Memory**: LangChain supports various tools and memory systems to enhance functionalities. You can integrate tools for web scraping, databases, or even custom logic.\\n\\n7. **Experiment and Iterate**: Play around with different prompts, models, and configurations to see how they affect the output. LangChain is designed to be flexible, so you can tailor it to your needs.\\n\\n8. **Deployment**: Once you’re satisfied with your application, consider deploying it using a web framework or an API to make it accessible.\\n\\nBy following these steps, you can effectively harness the power of LangChain to create versatile language model applications.', response_metadata={'token_usage': {'completion_tokens': 417, 'prompt_tokens': 44, 'total_tokens': 461, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-625458eb-8fdd-4bb6-91a9-f0f866207770-0', usage_metadata={'input_tokens': 44, 'output_tokens': 417, 'total_tokens': 461})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "26159af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26159af7",
        "outputId": "47b7e252-3a52-4bd7-e856-f912203e9d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 equals 4.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 24, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-232b3879-87a1-4eb4-a01c-063671a0efd7-0', usage_metadata={'input_tokens': 24, 'output_tokens': 9, 'total_tokens': 33})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a706e6",
      "metadata": {},
      "source": [
        "# Using Langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2deb534a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define the state using Pydantic\n",
        "class QuestionState(TypedDict):\n",
        "    question: str\n",
        "    topic: str\n",
        "    answer: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ef459ac4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "def classify_topic(state: QuestionState) -> QuestionState:\n",
        "    topic = chain.invoke({\"question\": state['question']})\n",
        "    return {\"topic\":topic}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a26a6982",
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_graph(state: QuestionState) -> str:\n",
        "    topic = (state['topic'] or \"\").lower()\n",
        "    if \"gemini\" in topic:\n",
        "        return \"gemini\"\n",
        "    elif \"langchain\" in topic:\n",
        "        return \"langchain\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "def run_langchain(state: QuestionState) -> QuestionState:\n",
        "    answer = langchain_chain.invoke({\"question\": state['question']}).content\n",
        "    return {\"answer\":answer}\n",
        "\n",
        "def run_gemini(state: QuestionState) -> QuestionState:\n",
        "    answer = gemini_chain.invoke({\"question\": state['question']}).content\n",
        "    return {\"answer\":answer}\n",
        "\n",
        "def run_general(state: QuestionState) -> QuestionState:\n",
        "    answer = general_chain.invoke({\"question\": state['question']}).content\n",
        "    return {\"answer\":answer}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8babb15c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph, END\n",
        "\n",
        "graph = StateGraph(QuestionState)\n",
        "graph.add_node(\"classify\", classify_topic)\n",
        "graph.add_node(\"langchain\", run_langchain)\n",
        "graph.add_node(\"gemini\", run_gemini)\n",
        "graph.add_node(\"general\", run_general)\n",
        "\n",
        "graph.add_conditional_edges(\"classify\", route_graph,{\"gemini\":\"gemini\",\"general\":\"general\",\"langchain\":'langchain'})\n",
        "graph.add_edge(START,\"classify\")\n",
        "graph.add_edge(\"general\",END)\n",
        "graph.add_edge(\"langchain\",END)\n",
        "graph.add_edge(\"gemini\",END)\n",
        "\n",
        "chain = graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c767d2a9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(chain.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "607c7a41",
      "metadata": {},
      "outputs": [
        {
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many employees are there?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m ):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1661\u001b[0m         ):\n\u001b[0;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mclassify_topic\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclassify_topic\u001b[39m(state: QuestionState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuestionState:\n\u001b[1;32m----> 4\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:topic}\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1936\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1937\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1938\u001b[0m     config,\n\u001b[0;32m   1939\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1940\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1941\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1942\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1943\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1947\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1661\u001b[0m         ):\n\u001b[0;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mclassify_topic\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclassify_topic\u001b[39m(state: QuestionState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuestionState:\n\u001b[1;32m----> 4\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:topic}\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1936\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1937\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1938\u001b[0m     config,\n\u001b[0;32m   1939\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1940\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1941\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1942\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1943\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1947\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
            "    \u001b[1;31m[... skipping similar frames: classify_topic at line 4 (367 times), RunnableSeq.invoke at line 408 (367 times), RunnableCallable.invoke at line 184 (367 times), Pregel.invoke at line 1936 (367 times), run_with_retry at line 40 (367 times), Pregel.stream at line 1656 (367 times), PregelRunner.tick at line 167 (367 times)]\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1661\u001b[0m         ):\n\u001b[0;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mclassify_topic\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclassify_topic\u001b[39m(state: QuestionState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuestionState:\n\u001b[1;32m----> 4\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:topic}\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1936\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1937\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1938\u001b[0m     config,\n\u001b[0;32m   1939\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1940\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1941\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1942\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1943\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1947\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:1605\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_modes:\n\u001b[0;32m   1602\u001b[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m c: stream\u001b[38;5;241m.\u001b[39mput(\n\u001b[0;32m   1603\u001b[0m         ((), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m, c)\n\u001b[0;32m   1604\u001b[0m     )\n\u001b[1;32m-> 1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[0;32m   1606\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1607\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[0;32m   1608\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1609\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[0;32m   1610\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[0;32m   1611\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[0;32m   1612\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[0;32m   1613\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1614\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[0;32m   1615\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[0;32m   1616\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[0;32m   1617\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1618\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1619\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[0;32m   1622\u001b[0m         submit\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m   1623\u001b[0m         put_writes\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39mput_writes,\n\u001b[0;32m   1624\u001b[0m         schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   1625\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[0;32m   1626\u001b[0m     )\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\loop.py:879\u001b[0m, in \u001b[0;36mSyncPregelLoop.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_metadata \u001b[38;5;241m=\u001b[39m saved\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_pending_writes \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    874\u001b[0m     [(\u001b[38;5;28mstr\u001b[39m(tid), k, v) \u001b[38;5;28;01mfor\u001b[39;00m tid, k, v \u001b[38;5;129;01min\u001b[39;00m saved\u001b[38;5;241m.\u001b[39mpending_writes]\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m saved\u001b[38;5;241m.\u001b[39mpending_writes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    877\u001b[0m )\n\u001b[1;32m--> 879\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39menter_context(\u001b[43mBackgroundExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m    881\u001b[0m     ChannelsManager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    882\u001b[0m )\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpush(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_suppress_interrupt)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\executor.py:53\u001b[0m, in \u001b[0;36mBackgroundExecutor.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m ExitStack()\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menter_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_executor_for_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks: \u001b[38;5;28mdict\u001b[39m[concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\contextlib.py:492\u001b[0m, in \u001b[0;36m_BaseExitStack.enter_context\u001b[1;34m(self, cm)\u001b[0m\n\u001b[0;32m    490\u001b[0m _cm_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(cm)\n\u001b[0;32m    491\u001b[0m _exit \u001b[38;5;241m=\u001b[39m _cm_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m\n\u001b[1;32m--> 492\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_cm_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_push_cm_exit(cm, _exit)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\runnables\\config.py:552\u001b[0m, in \u001b[0;36mget_executor_for_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get an executor for a config.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03m    Generator[Executor, None, None]: The executor.\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    551\u001b[0m config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mContextThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_concurrency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m executor\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\thread.py:151\u001b[0m, in \u001b[0;36mThreadPoolExecutor.__init__\u001b[1;34m(self, max_workers, thread_name_prefix, initializer, initargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_workers \u001b[38;5;241m=\u001b[39m max_workers\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mSimpleQueue()\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idle_semaphore \u001b[38;5;241m=\u001b[39m \u001b[43mthreading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSemaphore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\threading.py:425\u001b[0m, in \u001b[0;36mSemaphore.__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemaphore initial value must be >= 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond \u001b[38;5;241m=\u001b[39m \u001b[43mCondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m value\n",
            "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ],
      "source": [
        "for step in chain.stream(\n",
        "    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"\n",
        "):\n",
        "    print(step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff40bcb3",
      "metadata": {
        "id": "ff40bcb3"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You've now learned how to add routing to your composed LCEL chains.\n",
        "\n",
        "Next, check out the other how-to guides on runnables in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927b7498",
      "metadata": {
        "id": "927b7498"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
