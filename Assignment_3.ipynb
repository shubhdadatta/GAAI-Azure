{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Agentic RAG System with Azure OpenAI, Pinecone & LangGraph\n\nThis notebook demonstrates an **Agentic Retrieval-Augmented Generation (RAG) system** following the Assignment 3 requirements:\n\n- Retrieve relevant KB snippets from Pinecone\n- Generate initial answer using Azure GPT-4 mini\n- Critique answer completeness\n- Refine query & answer if needed\n- Log all steps with MLflow\n- Visualize workflow"},{"cell_type":"code","metadata":{},"source":"# ------------------------------- Imports -------------------------------\nimport os\nimport json\nimport mlflow\nfrom pydantic import BaseModel\nfrom typing import List, Dict\nfrom langgraph.graph import StateGraph, START, END\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Azure LLM\nfrom langchain.chat_models import AzureChatOpenAI\n\n# Pinecone embeddings & index\n# from pinecone_client import index, embeddings"},{"cell_type":"markdown","metadata":{},"source":"## Step 1: Setup MLflow\n\nSet the MLflow tracking URI and create an experiment for logging workflow runs."},{"cell_type":"code","metadata":{},"source":"mlflow.set_tracking_uri("http://localhost:5000")  # adjust as needed\nmlflow.set_experiment("Agentic_RAG_Workflow")"},{"cell_type":"markdown","metadata":{},"source":"## Step 2: Define RAG State\n\nUse `Pydantic` for a structured state object that tracks the query, retrieved snippets, answers, critique flag, refined query, and final answer."},{"cell_type":"code","metadata":{},"source":"class RAGState(BaseModel):\n    query: str\n    retrieved_snippets: List[Dict] = []\n    initial_answer: str = ""\n    critique_complete: bool = False\n    refined_query: str = ""\n    refined_answer: str = ""\n    final_answer: str = """},{"cell_type":"markdown","metadata":{},"source":"## Step 3: Index KB into Pinecone\n\nLoad KB JSON (~30 entries) and store vectors into Pinecone using Azure embeddings. Each document is stored with metadata including `question`, `answer_snippet`, `source`, and `last_updated`."},{"cell_type":"code","metadata":{},"source":"DATA_FILE = "self_critique_loop_dataset.json"\n\ndef index_kb():\n    """Load KB JSON and index into Pinecone."""\n    with open(DATA_FILE, "r") as f:\n        kb_data = json.load(f)\n\n    for i, entry in enumerate(kb_data):\n        doc_text = f"Q: {entry['question']} A: {entry['answer_snippet']}"\n        vector = embeddings.embed_query(doc_text)\n        if not vector:\n            continue\n        try:\n            index.upsert([(\n                entry["doc_id"],\n                vector,\n                {\n                    "question": entry["question"],\n                    "answer_snippet": entry["answer_snippet"],\n                    "source": entry["source"],\n                    "confidence": entry.get("confidence_indicator", "unknown"),\n                    "last_updated": entry.get("last_updated", "")\n                }\n            )])\n        except Exception as e:\n            print(f"Indexing error for {entry['doc_id']}: {e}")\n    print("KB indexing complete.")\n\n# index_kb()  # Uncomment to index KB"},{"cell_type":"markdown","metadata":{},"source":"## Step 4: Define Utility Function for Retrieval\n\nQuery Pinecone and return **top-k distinct results**, avoiding duplicates based on text content."},{"cell_type":"code","metadata":{},"source":"def query_distinct_text(index, embeddings, query, top_k=5, normalize=True):\n    try:\n        query_vector = embeddings.embed_query(query)\n    except Exception as e:\n        print(f"❌ Error generating embedding: {e}")\n        return []\n\n    try:\n        query_res = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n    except Exception as e:\n        print(f"❌ Error querying index: {e}")\n        return []\n\n    matches = query_res.get("matches", [])\n    if not matches:\n        print("⚠️ No matches found.")\n        return []\n\n    distinct_results = []\n    seen_texts = set()\n\n    for match in matches:\n        metadata = match.get("metadata", {})\n        text = metadata.get("answer_snippet", "")\n        if not text:\n            continue\n        key = " \".join(text.lower().split()) if normalize else text.strip()\n        if key not in seen_texts:\n            seen_texts.add(key)\n            distinct_results.append({\n                "id": match.get("id"),\n                "score": match.get("score", 0.0),\n                "metadata": metadata\n            })\n\n    if not distinct_results and matches:\n        first = matches[0]\n        distinct_results = [{\n            "id": first.get("id"),\n            "score": first.get("score", 0.0),\n            "metadata": first.get("metadata", {})\n        }]\n    return distinct_results"},{"cell_type":"markdown","metadata":{},"source":"## Step 5: Initialize Azure LLM\n\nUsing **Azure GPT-4 mini** with deterministic output (temperature=0)."},{"cell_type":"code","metadata":{},"source":"llm_model_name = "gpt-4o-mini"\nllm = AzureChatOpenAI(\n    model=llm_model_name,\n    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],\n    api_version=os.environ.get("AZURE_OPENAI_API_VERSION", "2024-02-01"),\n    temperature=0\n)"},{"cell_type":"markdown","metadata":{},"source":"## Step 6: Define Workflow Nodes\n\n1. **Retriever Node** → fetch top 5 snippets\n2. **LLM Answer Node** → generate initial answer\n3. **Critique Node** → decide COMPLETE or REFINE\n4. **Refinement Node** → rewrite query, retrieve extra snippet, generate concise answer"},{"cell_type":"code","metadata":{},"source":"def retriever_node(state: RAGState) -> RAGState:\n    print("\n=== NODE: Retriever ===")\n    state.retrieved_snippets = query_distinct_text(index, embeddings, state.query, top_k=5)\n    mlflow.log_text(str(state.retrieved_snippets), "retrieved_snippets.txt")\n    print("Current State:", state.model_dump())\n    return state\n\n\ndef llm_answer_node(state: RAGState) -> RAGState:\n    print("\n=== NODE: LLM Answer ===")\n    try:\n        snippets_text = "\n".join([\n            f"[{r['id']}] {r['metadata'].get('answer_snippet','')}" \n            for r in state.retrieved_snippets\n        ])\n        prompt = f"""\nAnswer the question below using ONLY the retrieved KB snippets with citations.\nDo not use your external knowledge unless absolutely necessary.\n\nQuestion:\n{state.query}\n\nKB Snippets:\n{snippets_text}\n"""\n        mlflow.log_text(prompt.strip(), "initial_answer_prompt.txt")\n        response = llm.invoke([\n            {"role": "system", "content": "You are a precise assistant using KB snippets exactly."},\n            {"role": "user", "content": prompt.strip()}\n        ])\n        state.initial_answer = response.content.strip()\n        mlflow.log_text(state.initial_answer, "initial_answer.txt")\n        print("LLM Answer:", state.initial_answer)\n    except Exception as e:\n        print(f"LLM Answer Error: {e}")\n    return state\n\ndef critique_node(state: RAGState) -> RAGState:\n    print("\n=== NODE: Critique ===")\n    state.critique_complete = len(state.retrieved_snippets) >= 3\n    mlflow.log_param("critique_complete", state.critique_complete)\n    print("Critique complete?", state.critique_complete)\n    return state\n\ndef refinement_node(state: RAGState) -> RAGState:\n    print("\n=== NODE: Refinement ===")\n    if state.critique_complete:\n        print("Refinement not required.")\n        return state\n    try:\n        refine_prompt = f"""\nYou are an assistant helping to retrieve missing KB info.\nOriginal query:\n{state.query}\n\nInitial answer:\n{state.initial_answer}\n\nRewrite the query in 1-2 sentences to target missing info.\n"""\n        mlflow.log_text(refine_prompt.strip(), "refine_query_prompt.txt")\n        refined_query_response = llm.invoke([\n            {"role": "system", "content": "You rewrite queries to target missing KB info."},\n            {"role": "user", "content": refine_prompt.strip()}\n        ])\n        state.refined_query = refined_query_response.content.strip()\n\n        existing_ids = {r['id'] for r in state.retrieved_snippets}\n        extra_snippets = query_distinct_text(index, embeddings, state.refined_query, top_k=6)\n        extra_snippets = [s for s in extra_snippets if s['id'] not in existing_ids]\n        if extra_snippets:\n            state.retrieved_snippets.append(extra_snippets[0])\n\n        snippets_text = "\n".join([\n            f"[{r['id']}] {r['metadata'].get('answer_snippet','')}" \n            for r in state.retrieved_snippets\n        ])\n        answer_prompt = f"""\nYou are a technical assistant. KB snippets below may not fully answer the query.\nIf insufficient, acknowledge and generate a concise 2-3 line

